{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {},
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycaret[full]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import libraries\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/dataset/weatherHistory.csv')\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Drop unnecessary columns\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "drop_cols = ['Formatted Date', 'Loud Cover', 'Precip Type', 'Daily Summary']\n",
    "df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True, errors='ignore')\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Handle missing values\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df.fillna(df.mean(numeric_only=True), inplace=True)\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Group similar weather summaries to reduce label noise\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def simplify_summary(summary):\n",
    "    summary = summary.lower()\n",
    "    if 'cloudy' in summary:\n",
    "        return 'cloudy'\n",
    "    elif 'clear' in summary:\n",
    "        return 'clear'\n",
    "    elif 'rain' in summary:\n",
    "        return 'rain'\n",
    "    elif 'fog' in summary:\n",
    "        return 'foggy'\n",
    "    elif 'drizzle' in summary:\n",
    "        return 'drizzle'\n",
    "    elif 'snow' in summary:\n",
    "        return 'snow'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "df['Simple_Summary'] = df['Summary'].apply(simplify_summary)\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Label encode target\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "le = LabelEncoder()\n",
    "df['Summary_encoded'] = le.fit_transform(df['Simple_Summary'])\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Drop the original summary columns\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df.drop(columns=['Summary', 'Simple_Summary'], inplace=True)\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Drop classes with fewer than 50 samples\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class_counts = df['Summary_encoded'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 50].index\n",
    "df_filtered = df[df['Summary_encoded'].isin(valid_classes)]\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "df_filtered.loc[:, 'ApparentTempDiff'] = df_filtered['Apparent Temperature (C)'] - df_filtered['Temperature (C)']\n",
    "df_filtered.loc[:, 'Humidity*Pressure'] = df_filtered['Humidity'] * df_filtered['Pressure (millibars)']\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PyCaret Classification\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from pycaret.classification import *\n",
    "\n",
    "clf = setup(data=df_filtered,\n",
    "            target='Summary_encoded',\n",
    "            session_id=123,\n",
    "            fix_imbalance=True,\n",
    "            fix_imbalance_method='smote',\n",
    "            verbose=False)\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare models and get the best one\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "best_model = compare_models()\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tune the best model\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "tuned_model = tune_model(best_model)\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate tuned_model directly:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "evaluate_model(tuned_model)"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predict on the same data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "predictions = predict_model(tuned_model, data=df_filtered)\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_sampled = df_filtered.sample(frac=0.3, random_state=123)\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performance metrics\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "y_true = df_filtered['Summary_encoded']\n",
    "y_pred = predictions['prediction_label']\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, average='weighted')\n",
    "rec = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(\" Performance Metrics:\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Confusion Matrix\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(cm, annot=False, cmap='Blues')\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Replace with your actual notebook filename\n",
    "notebook_path = \"your_notebook.ipynb\"\n",
    "\n",
    "with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "# Clean broken widget metadata\n",
    "for cell in nb.get(\"cells\", []):\n",
    "    metadata = cell.get(\"metadata\", {})\n",
    "    widgets = metadata.get(\"widgets\", {})\n",
    "    if isinstance(widgets, dict) and \"state\" not in widgets:\n",
    "        del metadata[\"widgets\"]\n",
    "\n",
    "with open(notebook_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(nb, f, indent=2)\n"
   ],
   "metadata": {},
   "outputs": []
  }
 ]
}